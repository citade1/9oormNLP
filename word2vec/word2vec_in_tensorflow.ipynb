{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uqSL7miVS757"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec(Skipgram, CBOW) TensorFlow로 구현"
      ],
      "metadata": {
        "id": "uATcOeX8N-eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skipgram dataset 만들기"
      ],
      "metadata": {
        "id": "fY-bNfY3o7Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset\n",
        "def generate_skipgram_dataset(original_data, window_size, batch_size):\n",
        "    \"\"\"\n",
        "    skipgram model에 들어갈 수 있는 데이터셋 생성.\n",
        "\n",
        "    parameters\n",
        "    ----------\n",
        "    original_data: list of strings. 데이터셋으로 바꿔야 할, 리스트로 이루어진 텍스트 데이터.\n",
        "    window_size: int. sliding window 크기\n",
        "    batch_size: int.\n",
        "\n",
        "    return\n",
        "    ------\n",
        "    dataset: model.fit에 바로 넣을 수 있는 데이터셋.\n",
        "    \"\"\"\n",
        "    # 단어 리스트\n",
        "    words = []\n",
        "    for sentence in original_data:\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    words = list(set(words))\n",
        "\n",
        "    # 단어를 index에 mapping\n",
        "    word2index = {w:i for i, w in enumerate(words)}\n",
        "    index2word = {i:w for i, w in enumerate(words)}\n",
        "\n",
        "    # skipgram data 만들기\n",
        "    pairs = []\n",
        "    for sentence in original_data:\n",
        "        tokens = sentence.split()\n",
        "        for idx, token in enumerate(tokens):\n",
        "            start = max(idx-window_size, 0)\n",
        "            end = min(idx+window_size+1, len(tokens))\n",
        "            for neighbor in tokens[start:end]:\n",
        "                if neighbor != token:\n",
        "                    pairs.append((word2index[token], word2index[neighbor]))\n",
        "\n",
        "    # dataset으로 변환\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((pairs[:, 0], pairs[:, 1]))\n",
        "    dataset = dataset.shuffle(len(pairs)).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "RNq7WYBEstlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wikipedia dump 다운로드\n",
        "!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzeNAkczev6T",
        "outputId": "1db8a50e-597b-4624-9a80-1a2c8717bd22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-15 09:40:17--  https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21869935187 (20G) [application/octet-stream]\n",
            "Saving to: ‘enwiki-latest-pages-articles.xml.bz2’\n",
            "\n",
            "es.xml.bz2            0%[                    ]  35.94M  4.70MB/s    eta 71m 43s^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_from_bz2_dump(file_path):\n",
        "    texts = []  # 추출된 텍스트를 저장할 리스트\n",
        "\n",
        "    with bz2.open(file_path, 'rb') as file:\n",
        "        for line in file:\n",
        "            # 각 줄을 파싱하기 위해 BeautifulSoup 사용\n",
        "            soup = BeautifulSoup(line, 'lxml')\n",
        "\n",
        "            # Wikipedia 문서의 텍스트 내용이 <text> 태그 안에 있음\n",
        "            for doc in soup.find_all('text'):\n",
        "                text = doc.get_text()\n",
        "                texts.append(text)\n",
        "\n",
        "    return texts\n",
        "\n",
        "# 함수 사용 예시\n",
        "file_path = 'enwiki-latest-pages-articles.xml.bz2'  # 다운로드한 파일의 경로\n",
        "texts = extract_text_from_bz2_dump(file_path)\n",
        "\n",
        "# 처음 5개의 문서 텍스트 출력\n",
        "for text in texts[:5]:\n",
        "    print(text[:100])  # 각 문서의 처음 100자만 출력\n"
      ],
      "metadata": {
        "id": "mzhafamhjAoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=False)\n",
        "\n",
        "train_data = dataset['train']\n",
        "train_text = train_data.map(lambda x:x['text'])\n",
        "train_text = train_text.numpy().decode('utf-8').lower()\n",
        "\n",
        "corpus = []\n",
        "for text in train_text:\n",
        "    words = text.split()\n",
        "    corpus.append(' '.join(words))\n",
        "\n",
        "train_dataset = generate_skipgram_dataset(corpus, window_size=5, batch_size=32)"
      ],
      "metadata": {
        "id": "QVSiUQVFU2AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skipgram\n",
        "skigram은 중심단어(target)로부터 주변단어(context)를 예측하는 구조로, 이를 통해 embedding matrix를 학습합니다.  \n",
        "  \n",
        "구현하는 방법은 주로 중심단어와 주변단어의 내적(dot product)을 계산하여 유사도를 구하는 것입니다. (코사인 특성상) 두 벡터가 유사할수록 내적의 값이 크고, 유사하지 않을수록 내적의 값이 작습니다. 이 내적값을 그대로 로짓으로 삼아 소프트맥스와 같은 함수에게 주어 각 클래스별 확률값을 예측하고, categorical crossentropy 손실함수를 사용하여 실제 확률분포와의 차이를 최소화합니다. 이를 통해 올바른 클래스를 예측할 수 있는 embedding matrix를 학습하게 됩니다."
      ],
      "metadata": {
        "id": "9mP8V55JpE2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                          embedding_dim,\n",
        "                                                          input_length=1)\n",
        "        self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                           embedding_dim,\n",
        "                                                           input_length=1)\n",
        "        self.dots = tf.keras.layers.Dot(axis=-1)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "    def call(self, pair):\n",
        "        target, context = pair\n",
        "        target_embedding = self.target_embedding(target)\n",
        "        context_embedding = self.context_embedding(context)\n",
        "        dots = self.dots([target_embedding, context_embedding])\n",
        "        flatten = self.flatten(dots)\n",
        "        return flatten"
      ],
      "metadata": {
        "id": "0YNVBI1Md9Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 300\n",
        "\n",
        "skipgram_model = SkipGramModel(vocab_size, embedding_dim)\n",
        "\n",
        "skipgram_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = tf.keras.losses.CategoricalCrossEntropy(from_logits=True),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "skipgram_dataset = generate_skipgram_dataset(data, vocab_size, embedding_dim)\n",
        "\n",
        "skipgram_model.fit(skipgram_dataset, epochs=10)\n",
        "s_history = skipgram_model.history()\n",
        "\n",
        "# 중심단어의 embedding matrix\n",
        "skipgram_embedding_matrix = skipgram_model.target_embedding.get_weights()[0]"
      ],
      "metadata": {
        "id": "ojchsQF7hyD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBOW\n",
        "cbow는 주변단어(context)로부터 중심단어(target)를 예측하는 구조입니다."
      ],
      "metadata": {
        "id": "vW1sYFQtiw0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cbow dataset\n",
        "def generate_cbow_dataset(original_data, window_size, batch_size):\n",
        "    \"\"\"\n",
        "    cbow model에 들어갈 수 있는 데이터셋 생성.\n",
        "\n",
        "    parameters\n",
        "    ----------\n",
        "    original_data: list of strings. 데이터셋으로 바꿔야 할, 리스트로 이루어진 텍스트 데이터.\n",
        "    window_size: int. sliding window 크기\n",
        "    batch_size: int.\n",
        "\n",
        "    return\n",
        "    ------\n",
        "    dataset: model.fit에 바로 넣을 수 있는 데이터셋.\n",
        "    \"\"\"\n",
        "    # 단어 리스트\n",
        "    words = []\n",
        "    for sentence in original_data:\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    words = list(set(words))\n",
        "\n",
        "    # 단어를 index에 mapping\n",
        "    word2index = {w:i for i, w in enumerate(words)}\n",
        "    index2word = {i:w for i, w in enumerate(words)}\n",
        "\n",
        "    # cbow data 만들기\n",
        "    data = []\n",
        "    for sentence in original_data:\n",
        "        tokens = [word2index[word] for word in sentence.split()]\n",
        "        for idx, token in enumerate(tokens):\n",
        "            start = max(idx-window_size, 0)\n",
        "            end = min(idx+window_size+1, len(tokens))\n",
        "            context = [neighbor for neighbor in tokens[start:end] if neighbor != token]\n",
        "            if len(context)>0:\n",
        "                data.append((context, token))\n",
        "\n",
        "    contexts, targets = zip(*data)\n",
        "    contexts = tf.keras.preprocessing.sequence.pad_sequences(contexts, padding='post')\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((contexts, targets))\n",
        "    dataset = dataset.shuffle(len(targets)).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "rN9O-441etl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CBOW 모델 정의\n",
        "class CBOWModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                   embedding_dim,\n",
        "                                                   input_length=None)\n",
        "        self.global_average_pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, context):\n",
        "        context_embedding = self.embedding(context)\n",
        "        pooled_context = self.global_average_pooling(context_embedding)\n",
        "        predictions = self.dense(pooled_context)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "pWL5MVHIcKAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cbow_model = CBOWModel(vocab_size, embedding_dim)\n",
        "\n",
        "cbow_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "cbow_dataset = generate_cbow_dataset(data, vocab_size, embedding_dim)\n",
        "\n",
        "cbow_model.fit(cbow_dataset, epochs=10)\n",
        "c_history = cbow_model.history()\n",
        "\n",
        "# 주변단어(context)의 embedding matrix\n",
        "cbow_embedding_matrix = cbow_model.embedding.get_weights()[0]"
      ],
      "metadata": {
        "id": "CtxfQ_PodNac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### skipgram & cbow by word2vec function"
      ],
      "metadata": {
        "id": "9aMdHoBjtHPi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3O-a2lBte4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison via visualization (tensorflow vs word2vec)"
      ],
      "metadata": {
        "id": "NbE-PRPXtikc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lf8soapEti6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}